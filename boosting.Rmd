---
title: "Boosting Project"
author: "Ashley Melanson"
output: 
    pdf_document:
        keep_tex: true
---

* UW ID: 20615537
* Kaggle public score: 0.18260 (RANK 10)
* Kaggle private score: 0.17612 (RANK 19)

# Summary

My final model contained 20 predictors (2 of which were derived). I retained many of the same feature engineering techniques from the random forest model project (e.g. replacement for missing values). The major difference for this model was that: \ 

(1) I needed to convert my categorical variables to dummy variables to work with xgboost - this affected the performance of one of my predictors (too many levels) \ 
(2) I needed to conduct a more manual grid search to properly tune the parameters (as there were many combinations) \ 
(3) I needed to conduct a cross validation of the optimal number of rounds needed to fit the boosting model \ 

## Preprocessing

```{r}
# load in the data
load('house.Rdata')
```

```{r message=FALSE, warning=FALSE}
# libraries used
library(Metrics)
require(Matrix)
library(caret)
library(dplyr)
library(reshape2)
library(xgboost)
library(ggplot2)
```

Must make sure that levels between dtrain and dtest match (for predictive purposes)

```{r}
levels(dtest$EXTWALL) <- levels(dtrain$EXTWALL)
levels(dtest$ZIPCODE) <- levels(dtrain$ZIPCODE)
levels(dtest$GRADE) <- levels(dtrain$GRADE)
levels(dtest$QUADRANT) <- levels(dtrain$QUADRANT)
levels(dtest$CNDTN) <- levels(dtrain$CNDTN)
levels(dtest$WARD) <- levels(dtrain$WARD)
levels(dtest$HEAT) <- levels(dtrain$HEAT)
levels(dtest$ROOF) <- levels(dtrain$ROOF)
levels(dtest$STYLE) <- levels(dtrain$STYLE)
```

We also need to do some releveling of the categorical variables (where possible) so that the RF Model can make more accurate decisions.

```{r}
# We need some releveling:
dtrain$CNDTN = factor(dtrain$CNDTN, levels = c('Poor', 'Fair', 'Average', 'Good',
                                               'Very Good', 'Excellent'), ordered=TRUE)
  
dtrain$GRADE = factor(dtrain$GRADE,
                      levels = c("Low Quality", "Fair Quality", "Average",
                                  "Above Average", "Good Quality", "Very Good",
                                  "Excellent", "Superior", "Exceptional-A", "Exceptional-B",
                                 "Exceptional-C", "Exceptional-D"), ordered=TRUE)

```

### Missing Data

```{r}
# Missing Values - TRAINING SET
isNA <- sapply(dtrain, function(x) sum(is.na(x)))
isNA[isNA > 0]
```

* QUADRANT: I replaced the missing values with a new level, "NA"

```{r}
  # converting missing values to level "NA"
  dtrain$QUADRANT <- as.character(dtrain$QUADRANT)
  dtrain$QUADRANT[is.na(dtrain$QUADRANT)] <- "NA"
  dtrain$QUADRANT <- as.factor(dtrain$QUADRANT)
```

* ASSESSMENT_SUBNBHD: I replaced the missing values with their ASSESSMENT_NBHD value

```{r}
  # converting missing values to level according to ASSESSMENT_NBHD
  dtrain$ASSESSMENT_SUBNBHD <- as.character(dtrain$ASSESSMENT_SUBNBHD)
  dtrain$ASSESSMENT_NBHD <- as.character(dtrain$ASSESSMENT_NBHD)
  
  dtrain$ASSESSMENT_SUBNBHD[is.na(dtrain$ASSESSMENT_SUBNBHD)] <-
    dtrain$ASSESSMENT_NBHD[is.na(dtrain$ASSESSMENT_SUBNBHD)]
  
  dtrain$ASSESSMENT_SUBNBHD <- as.factor(dtrain$ASSESSMENT_SUBNBHD)
  dtrain$ASSESSMENT_NBHD <- as.factor(dtrain$ASSESSMENT_NBHD)
```

* YR_RMDL: I replaced the missing values with the value in EYB (closest by definition from variable_description.txt)

```{r}
  # setting missing values to EYB
  dtrain$YR_RMDL[is.na(dtrain$YR_RMDL)] <- dtrain$EYB[is.na(dtrain$YR_RMDL)]
```

* STORIES : I replaced the missing values with information from STYLE

```{r}
  # Check the subset of NA stories and use the STYLE to fill the values in
  dtrain$STORIES[469] <- 2
  dtrain$STORIES[3852] <- 2.5
  dtrain$STORIES[6626] <- 2
```

* AYB : I replaced the missing values with a formula : EYB - Avg Distance between AYB and EYB (Which is 30)

```{r}
  # We replace our NA AYB values using this formula,
  # 30 is the average distance between EYB and AYB
  dtrain$AYB[is.na(dtrain$AYB)] <- dtrain$EYB[is.na(dtrain$AYB)] - 30
```

### Transformation


Introducing new variables, that were previously introduced in my smoothing model. They were found to have strong effects on the model's ability to predict price, so we will try to use them for the RF Model.



* ASSESSMENT_NBHD and ASSESSMENT_SUBNBHD: Attempt to relevel these variables based on order of mean(PRICE)
This seemed to work very well, as their importance scores in the RF Model significantly jumped

```{r}
# create the sorted dataframe
nbhd_df <-
dtrain %>%
  group_by(ASSESSMENT_NBHD) %>%
  summarise_at(vars(PRICE), list(mean)) %>% arrange(PRICE)

# order the variable based on the new dataframe
dtrain$ASSESSMENT_NBHD <-
  factor(dtrain$ASSESSMENT_NBHD,
         levels=unique(nbhd_df$ASSESSMENT_NBHD[order(nbhd_df$PRICE)]),
         ordered=TRUE)
```

```{r}
# create the sorted dataframe
subnbhd_df <-
dtrain %>%
  group_by(ASSESSMENT_SUBNBHD) %>%
  summarise_at(vars(PRICE), list(mean)) %>% arrange(PRICE)

# order the variable based on the new dataframe
dtrain$ASSESSMENT_SUBNBHD <-
  factor(dtrain$ASSESSMENT_SUBNBHD,
         levels=unique(subnbhd_df$ASSESSMENT_SUBNBHD[order(subnbhd_df$PRICE)]))
```

I created the sub-dataframes below so that I would be looking at only these features when training the boosting model. These predictors were taken from my Random Forest Model.

```{r}

dtrain_sub <-
    dtrain %>%
    select(PRICE, ROOMS, SALEDATE, AGE_SOLD, AGE_SOLD_2, YR_RMDL,
           WARD, BATHRM, GBA, LANDAREA, FIREPLACES, EYB, AYB,
           BEDRM, LATITUDE, LONGITUDE, ZIPCODE, GRADE, CNDTN,
           ASSESSMENT_NBHD, ASSESSMENT_SUBNBHD)
  
  # applying the log transform
  dtrain_sub$PRICE <- log(dtrain_sub$PRICE)
  # just filling PRICE with zeros for now
  # dtest_sub$PRICE <- replicate(2472,0)
```

I then converted the categorical variables into dummy variables using dummyVars from Caret.

```{r}
  # create the dummy variables (encoding of categorical variables)
  dmy_train <- dummyVars(" ~ .", data=dtrain_sub)
  dtrain_encode <- data.frame(predict(dmy_train, newdata = dtrain_sub))

  
 # dmy_test <- dummyVars(" ~ .", data=dtest_sub)
 # dtest_encode <- data.frame(predict(dmy_test, newdata = dtest_sub))
  
  # create the xgb matrices
  dtrain_xgb = xgb.DMatrix(data = as.matrix(dtrain_encode %>% select(-PRICE)), 
                           label = dtrain_encode$PRICE)
 # dtest_xgb = xgb.DMatrix(data =  as.matrix(dtest_encode %>% select(-PRICE)), label = dtest_encode$PRICE)
```

## Model Building

Main Random Forests package used: `xgboost`
I came up with the final model below:

```{r include=FALSE}
  op_params <- list(booster = "gbtree", eta=0.1, gamma=0, max_depth=4, 
                    min_child_weight=1, subsample=1, colsample_bytree=1)

  boost.model <- xgboost(params = op_params, data = dtrain_xgb, nrounds = 74,
                         print_every_n = 10, maximize = F, eval_metric = "rmsle")
```

More details on tuning below:

(1) I first needed to figure out how many rounds was appropriate I ran cross validation through the xgboost package, using default parameters. I found that the optimal number of rounds was 74 (However in my kaggle result, I used 149 after running on different parameters)

```{r}
# default parameters
params <- list(booster = "gbtree", eta=0.3, gamma=0, max_depth=6, 
               min_child_weight=1, subsample=1, colsample_bytree=1)

xgbcv <- xgb.cv(params = params, data=dtrain_xgb, nrounds=150, nfold=5, showsd=T,
                stratfied=T, print_every_n = 10, early_stopping_rounds = 20, 
                maximize = F, eval_metric = "rmsle")
```

I would also periodically look at the importance of the variables in my model.

```{r}
importance <- xgb.importance(model = boost.model)
head(importance)
```

(2) To find the optimal parameters, I did a "partially" manual grid search. While setting all other parameters to their default, I would loop through different value of a specific parameter. I would then look at the convergence of those values, and the lowest rmsle.

I got that: eta = 0.4, colsample_bylevel = 1, max_depth = 4, min_child_weight = 1, gamma = 0

* eta : learning (or shrinkage) parameter, cntrols how much info from a new tree will be used for boosting. If eta = 1 then we use all info from new tree

* colsample_bylevel : ust like Random Forests, some times it is good to look only at a few variables to grow each new node in a tree

* max_depth : Controls the maximum depth of the trees. Deeper trees have more terminal nodes and fit more data. Convergence also requires less trees if we grow them deep

* gamma : Controls the minimum reduction in the loss function required to grow a new node in a tree. This parameter is sensitive to the scale of the loss function, which will be linked to the scale of your response variable.

* min_child_weight : Controls the minimum number of observations (instances) in a terminal node. The minimum value for this parameter is 1, which allows the tree to have terminal nodes with only one observation.

```{r}
set.seed(1)

# = eta candidates = #
eta=c(0.05,0.1,0.2,0.3,0.4,0.5,1)
# = colsample_bylevel candidates = #
cs=c(1/3,2/3,1)
# = max_depth candidates = #
md=c(2,4,6,10)
# = sub_sample candidates = #
ss=c(0.25,0.5,0.75,1)
# = min_child_weights candidates = #
mcw=c(1,10,100,400)
# = gamma candidates = #
gamma=c(0,0.1,1,10,100)

# = standard model is the second value  of each vector above = #
standard=c(2,2,3,2,1,1)

```

```{r}
conv_eta = matrix(NA,74,length(eta))

for(i in 1:length(eta)) {
  params <- list(booster = "gbtree", eta=eta[i], gamma=gamma[standard[6]],
                 max_depth=md[standard[3]],
                 min_child_weight=mcw[standard[5]],
                 subsample=ss[standard[4]],
                 colsample_bylevel=cs[standard[2]])
  
  model <- xgboost(params = params, data = dtrain_xgb, verbose = 0,
                   nrounds = 74, maximize = F, eval_metric = "rmsle")
  conv_eta[,i] = model$evaluation_log$train_rmsle
}
```

```{r}
conv_eta = data.frame(iter=1:74, conv_eta)
conv_eta = melt(conv_eta, id.vars = "iter")
ggplot(data = conv_eta) + geom_line(aes(x = iter, y = value, color = variable))
```
